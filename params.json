{
  "name": "Deep Remix",
  "tagline": "Remixing Music with Deep Learning",
  "body": "# Any song. Remixed by any artist. Automatically.\r\nUsing bleeding edge research and technologies, we are going to advance the state of the art in automated music creation.\r\nBy the time this project is over, we either will have passed an auditory Turing Test or our computers will have died trying.\r\n\r\n# Description\r\nResearch into deep learning is progressing extremely quickly, with new state of the art results seemingly being obtained at increasingly faster intervals. But one area of study that has been sorely neglected has been generating realistic sounds, especially music. I suspect that is because there is limited commercial potential in the space, but I digress.\r\n\r\nWith Deep Remix, I want to take the latest research in all fields of machine learning and try to apply them to generating sounds with the aim of making them indistinguishable from music made by humans. The most promising research I think is in the generative adversarial network, or GAN, framework, which has had extremely good results in producing realistic images of objects. Of course, with music being sequential, the obvious choice for the generative model’s architecture is the long short term memory, or LSTM, network, a type of recurrent neural network (RNN) that has been very successful in generating realistic text. I suspect that a combination of the two can result in a generative model that can produce realistic sequential data, of which music is one type.\r\n\r\nSome work has been done previously (https://github.com/jisungk/deepjazz and https://github.com/MattVitelli/GRUV), but these were using only RNNs, not GAN’s, and the initialization of these was with random noise, not anything that resembles music.\r\n\r\nI am proposing that instead of trying to generate music completely from scratch (at least not to start off with), the input to the network is a previous song and the output is a “remix” of it, similar to Neural Artistic Style (https://arxiv.org/abs/1508.06576). In addition, if we can place this in a GAN architecture, theoretically, our outputs should be much closer to sounding like music than previous attempts.\r\n\r\nI also have a few theories about using hierarchical networks to enforce musical pattern repetition within the output as well as potentially augmenting the network with pre-trained “note/sound vectors”, which would be similar to word vectors (https://arxiv.org/abs/1310.4546). We will hopefully get the chance to test some of these throughout the semester.\r\n\r\nYou can learn more about RNNs by reading Andrej Karpathy’s excellent blog post on the subject: http://karpathy.github.io/2015/05/21/rnn-effectiveness/. Unfortunately, no one really has a good write-up on GANs, but the original paper is relatively simple to understand at a high level: https://arxiv.org/abs/1406.2661. \r\n\r\n# Timeline and Deliverables\r\n A more specific timeline can be hashed out once I get a better sense of the number of people who help out. For now, I will just list the major milestones I think should be hit.\r\n\r\n1. Get an LSTM to produce something that sounds like music\r\n2. Place the LSTM in a GAN framework to try to improve on just the LSTM\r\n3. Test all of our theories to try and improve on our best previous result\r\n4. Moonshot: generate music that passes an auditory Turing Test\r\n\r\n# Distribution of Work\r\ntbd\r\n\r\n# Languages and Frameworks\r\nPython! Keras to start, Tensorflow to follow.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}